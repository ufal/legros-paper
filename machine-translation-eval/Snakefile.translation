mosesdecoder="corpus/scripts/mosesdecoder/scripts"
subword_segmentation="/lnet/troja/projects/neuralpiece/subword-segmentation"
sentencepiece="/lnet/troja/projects/neuralpiece/evaluation/sentencepiece/build/src"
marian_home="/lnet/troja/projects/hplt/marian-dev/build"


VOCAB_SIZES = [2, 4, 8, 16, 32]
TYPES = ["bpe", "unigram"]
EXPERIMENTS = ["ours", "subword"]

import glob

rule joint_sentencepiece_model:
    input:
        glob.glob("plain_data/train.*")
    output:
        "subword-{model_type}-{kwords}k/{model_type}.{kwords}k.model",
        "subword-{model_type}-{kwords}k/{model_type}.{kwords}k.vocab",
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    shell:
        """
        DIR=subword-{wildcards.model_type}-{wildcards.kwords}k
        mkdir -p $DIR
        {sentencepiece}/spm_train \
            --input={input[0]},{input[1]} \
            --model_prefix=$DIR/{wildcards.model_type}.{wildcards.kwords}k \
            --vocab_size={wildcards.kwords}000 \
            --model_type={wildcards.model_type} \
            --num_threads={threads} \
            --input_sentence_size=10000000 \
            --shuffle_input_sentence=true \
        """

rule apply_sentencepiece:
    input:
        data="plain_data/{split}.{lng}",
        model="subword-{model_type}-{kwords}k/{model_type}.{kwords}k.model",
    output:
        "subword-{model_type}-{kwords}k/{split}.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
    shell:
        "cat {input.data} | {sentencepiece}/spm_encode --model={input.model} > {output}"


rule pretokenize:
    input:
        "plain_data/{split}.{lng}"
    output:
        "pretokenized/{split}.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
    run:
       with open(input[0], "r") as f:
            with open(output[0], "w") as g:
                for line in f:
                    char_list = ["▁"]
                    for char in line.strip():
                        if char == " ":
                            char_list.append(" ")
                            char_list.append("▁")
                            continue
                        if char_list[-1] != "▁" and char_list[-1].isalpha() and not char.isalpha():
                            char_list.append(" ")
                        if char_list[-1] != "▁" and not char_list[-1].isalpha() and char.isalpha():
                            char_list.append(" ")
                        char_list.append(char)
                    print("".join(char_list), file=g)


rule fasttext:
    input:
        "pretokenized/train.{lng}"
    params:
        prefix="fasttext/{lng}.ft"
    output:
        "fasttext/{lng}.ft",
        "fasttext/{lng}.ft.vocab",
        "fasttext/{lng}.ft.out_inv.txt",
        "fasttext/{lng}.ft.txt"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    shell:
        """
        mkdir -p fasttext
        {subword_segmentation}/scripts/train_fasttext.py --epochs 30 --num-threads {threads} {input} {params.prefix}
        """

rule allowed_init_list_from_sentencepiece:
    input:
        model="subword-{model_type}-{kwords}k/{model_type}.{kwords}k.model",
        ftvocab="fasttext/{lng}.ft.vocab"
    output:
        "fasttext/allowed.{model_type}-{kwords}k.{lng}.init"
    resources:
        mem="4G",
        cpus_per_task=2
    shell:
        """
        paste {input.ftvocab} <(
                sed 's/^▁//' {input.ftvocab} | \
                {sentencepiece}/spm_encode --model={input.model} ) \
        > {output}
        """

rule subword_embeddings:
    input:
        allowed="fasttext/allowed.{segm_type}-{kwords}k.{lng}.init",
        pseudoinverse="fasttext/{lng}.ft.out_inv.txt",
        word_embeddings="fasttext/{lng}.ft.txt",
        data="pretokenized/train.{lng}"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    params:
        epochs=10,
        workdir=lambda wildcards, output: "subword_embs-{}-{}k.{}".format(
            wildcards.segm_type, wildcards.kwords, wildcards.lng)
    output:
        "subword_embs-{segm_type}-{kwords}k.{lng}/segmentations.9",
        "subword_embs-{segm_type}-{kwords}k.{lng}/subwords.9",
        "subword_embs-{segm_type}-{kwords}k.{lng}/unigram_stats.9",
        "subword_embs-{segm_type}-{kwords}k.{lng}/bigram_stats.9",
    shell:
        # note this assumes params.workdir is a direct subdirectory
        """
        mkdir -p {params.workdir}
        {subword_segmentation}/build/train_subword_embeddings \
            {input.word_embeddings} \
            {input.data} \
            --fastext-output-pseudoinverse {input.pseudoinverse} \
            --allowed-substrings {input.allowed} \
            --epochs {params.epochs} \
            --output-directory {params.workdir}
        """

rule bigram_segment:
    input:
        bigram_stats="subword_embs-{segm_type}-{kwords}k.{lng}/bigram_stats.9",
        unigram_stats="subword_embs-{segm_type}-{kwords}k.{lng}/unigram_stats.9",
        data="pretokenized/{split}.{lng}"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    output:
        "ours-{segm_type}-{kwords}k/{split}.{lng}"
    shell:
        """
        mkdir -p segmented.{wildcards.segm_type}-{wildcards.kwords}k
        {subword_segmentation}/build/bigram_segment \
            {input.bigram_stats} \
            {input.unigram_stats} \
            < {input.data} | sed 's/@@//g' > {output}
        """

rule train_marian:
    input:
        train_src="{segmentation}-{segm_type}-{kwords}k/train.{src}",
        train_tgt="{segmentation}-{segm_type}-{kwords}k/train.{tgt}",
        dev_src="{segmentation}-{segm_type}-{kwords}k/dev.{src}",
        dev_tgt="{segmentation}-{segm_type}-{kwords}k/dev.{tgt}",
    output:
        model="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/model.npz",
        vocab="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/vocab.yml",
        decoder_config="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/model.npz.decoder.yml",
        log="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/train.log",
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:2"
    shell:
        """
        mkdir -p models/{wildcards.src}-{wildcards.tgt}-{wildcards.segm_type}-{wildcards.kwords}k
        hostname
        {marian_home}-`hostname`/marian \
            --model {output.model} \
            --log {output.log} \
            --vocabs {output.vocab} {output.vocab} \
            --config config.yml \
            --devices 0 1 \
            --train-sets {input.train_src} {input.train_tgt} \
            --valid-sets {input.dev_src} {input.dev_tgt}
        """

rule translate:
    input:
        decoder_config="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/model.npz.decoder.yml",
        test_src="{segmentation}-{segm_type}-{kwords}k/{split}.{src}",
    output:
        test_out="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/{split}.out"
    resources:
        mem="120G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    shell:
        """
        {marian_home}-`hostname`/marian-decoder \
            --config {input.decoder_config} \
            < {input.test_src} | \
            sed 's/ //g;s/▁/ /g;s/^ *//g;s/ *$//g' \
            > {output.test_out}
        """

rule evaluate:
    input:
        test_out="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/{split}.out",
        test_reference="plain_data/{split}.{tgt}"
    output:
        bleu="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/{split}.bleu",
        chrf="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/{split}.chrf"
    run:
        import sacrebleu
        with open(input.test_out, encoding="utf8", errors="ignore") as f:
            hyps = [line.strip() for line in f]
        with open(input.test_reference, encoding="utf8", errors="ignore") as f:
            refs = [line.strip() for line in f]

        bleu = sacrebleu.corpus_bleu(hyps, [refs])
        chrf = sacrebleu.corpus_chrf(hyps, [refs])

        with open(output.bleu, "w") as f:
            f.write(str(bleu.score))
        with open(output.chrf, "w") as f:
            f.write(str(chrf.score))


rule done:
    input:
        test_bleu="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/test.bleu",
        test_chrf="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/test.chrf",
        dev_bleu="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/dev.bleu",
        dev_chrf="models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/dev.chrf",
    output:
        touch("models/{src}-{tgt}-{segmentation}-{segm_type}-{kwords}k/done")

