subword_segmentation="/lnet/troja/projects/neuralpiece/subword-segmentation"
sentencepiece="/lnet/troja/projects/neuralpiece/evaluation/sentencepiece/build/src"
marian_home="/lnet/troja/projects/hplt/marian-dev/build"

from languages import LNG1, LNG2

VOCAB_SIZES = [4, 8, 16]
TYPES = ["bpe", "unigram"]
EXPERIMENTS = ["embedding", "original"]
PRETOKS = ["words", "morf"]


rule all:
    input:
        expand(
            "models/{direction}-{pretok}-{type}-{vocab_size}k-{experiment}/done",
            pretok=PRETOKS,
            experiment=EXPERIMENTS,
            type=TYPES,
            vocab_size=VOCAB_SIZES,
            direction=[LNG1 + "-" + LNG2, LNG2 + "-" + LNG1])


rule joint_sentencepiece_model:
    input:
        "plain_data/train." + LNG1,
        "plain_data/train." + LNG2,
    output:
        "words-{model_type}-{kwords}k-original/{model_type}.{kwords}k.model",
        "words-{model_type}-{kwords}k-original/{model_type}.{kwords}k.vocab",
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    shell:
        """
        DIR=`dirname {output[0]}`
        mkdir -p $DIR
        {sentencepiece}/spm_train \
            --input={input[0]},{input[1]} \
            --model_prefix=$DIR/{wildcards.model_type}.{wildcards.kwords}k \
            --vocab_size={wildcards.kwords}000 \
            --model_type={wildcards.model_type} \
            --num_threads={threads} \
            --input_sentence_size=10000000 \
            --shuffle_input_sentence=true \
        """


rule apply_sentencepiece:
    input:
        data="plain_data/{split}.{lng}",
        model="words-{model_type}-{kwords}k-original/{model_type}.{kwords}k.model",
    output:
        "words-{model_type}-{kwords}k-original/{split}.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
    shell:
        "cat {input.data} | {sentencepiece}/spm_encode --model={input.model} > {output}"


def pretokenize_line(line):
    char_list = ["▁"]
    for char in line.strip():
        if char == " ":
            char_list.append(" ")
            char_list.append("▁")
            continue
        if char_list[-1] != "▁" and char_list[-1].isalpha() and not char.isalpha():
            char_list.append(" ")
        if char_list[-1] != "▁" and not char_list[-1].isalpha() and char.isalpha():
            char_list.append(" ")
        char_list.append(char)
    return "".join(char_list)


rule pretokenize:
    input:
        "plain_data/{split}.{lng}"
    output:
        "pretokenized/{split}.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
    run:
       with open(input[0], "r") as f:
            with open(output[0], "w") as g:
                for line in f:
                    print(pretokenize_line(line), file=g)


rule morfessor:
    input:
        "pretokenized/train.{lng}"
    output:
        "morfessor/{lng}.model",
        "morfessor/train.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
    run:
        import os
        os.makedirs("morfessor", exist_ok=True)
        os.system(f"morfessor-train --encoding=UTF-8 --logfile=morfessor/training.log --save {output[0]} -d ones {input[0]}")

        import morfessor
        io = morfessor.MorfessorIO()
        model = io.read_binary_model_file(output[0])

        with open(input[0], "r") as f:
            with open(output[1], "w") as g:
                for line in f:
                    line_tokens = []
                    for word in line.strip().split():
                        line_tokens.extend(model.viterbi_segment(word)[0])
                    print(" ".join(line_tokens), file=g)


rule sentencepiece_over_morfessor:
    input:
        "morfessor/train." + LNG1,
        "morfessor/train." + LNG2,
    output:
        model="morf-{model_type}-{kwords}k-original/{model_type}.{kwords}k.model",
        vocab="morf-{model_type}-{kwords}k-original/{model_type}.{kwords}k.vocab",
    resources:
        mem="40G",
        cpus_per_task=16
    shell:
        """
        DIR=`dirname {output.model}`
        mkdir -p $DIR
        VOCAB_SIZE={wildcards.kwords}000
        # Increase vocab size by 10% because we decrease it later
        VOCAB_SIZE=$((VOCAB_SIZE * 105 / 100))

        # Iterarate at most 4 times
        for i in 1 2 3 4 5 6; do
            # Decrease the vocab size by 10%
            VOCAB_SIZE=$((VOCAB_SIZE * 95 / 100))
            {sentencepiece}/spm_train \
                --input={input[0]},{input[1]} \
                --model_prefix=$DIR/{wildcards.model_type}.{wildcards.kwords}k \
                --vocab_size=$VOCAB_SIZE \
                --model_type={wildcards.model_type} \
                --num_threads={threads} \
                --train_extremely_large_corpus=true \
                --input_sentence_size=10000000 \
                --shuffle_input_sentence=true \
                --shrinking_factor 0.95 && break
        done
        """


rule apply_morfessor_and_sentencepiece:
    input:
        data="plain_data/{split}.{lng}",
        morf_model="morfessor/{lng}.model",
        sp_model="morf-{model_type}-{kwords}k-original/{model_type}.{kwords}k.model",
    output:
        "morf-{model_type}-{kwords}k-original/{split}.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
    run:
        import morfessor
        import sentencepiece as spm

        io = morfessor.MorfessorIO()
        morf_model = io.read_binary_model_file(input.morf_model)
        sp_model = spm.SentencePieceProcessor(model_file=input.sp_model)

        with open(input.data, "r") as f, open(output[0], "w") as g:
            for line in f:
                line_tokens = []
                pretok = pretokenize_line(line)
                for token in pretok.split():
                    if token.startswith("▁"):
                        prefix = "▁"
                        token = token[1:]
                    else:
                        prefix = ""
                    morf_tokens = morf_model.viterbi_segment(token)[0]
                    sp_tokens = []
                    for morf_token in morf_tokens:
                        sp_tokens.extend(
                            tok[1:] if tok.startswith("▁") else tok
                            for tok in sp_model.encode_as_pieces(morf_token))
                    if sp_tokens:
                        sp_tokens[0] = prefix + sp_tokens[0]
                        line_tokens.extend(sp_tokens)
                print(" ".join(line_tokens), file=g)


rule fasttext:
    input:
        "pretokenized/train.{lng}"
    params:
        prefix="fasttext/{lng}.ft"
    output:
        "fasttext/{lng}.ft",
        "fasttext/{lng}.ft.vocab",
        "fasttext/{lng}.ft.out_inv.txt",
        "fasttext/{lng}.ft.txt"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    shell:
        """
        mkdir -p fasttext
        {subword_segmentation}/scripts/train_fasttext.py --epochs 30 --num-threads {threads} {input} {params.prefix}
        """


rule allowed_init_list_from_sentencepiece:
    input:
        model="words-{model_type}-{kwords}k-original/{model_type}.{kwords}k.model",
        ftvocab="fasttext/{lng}.ft.vocab"
    output:
        "fasttext/allowed.words-{model_type}-{kwords}k.{lng}.init"
    resources:
        mem="4G",
        cpus_per_task=2
    shell:
        """
        paste {input.ftvocab} <(
                sed 's/^▁//' {input.ftvocab} | \
                {sentencepiece}/spm_encode --model={input.model} ) \
        > {output}
        """


rule allowed_init_list_from_morfessor:
    input:
        morf_model="morfessor/{lng}.model",
        sp_model="morf-{model_type}-{kwords}k-original/{model_type}.{kwords}k.model",
        ftvocab="fasttext/{lng}.ft.vocab"
    output:
        "fasttext/allowed.morf-{model_type}-{kwords}k.{lng}.init"
    resources:
        mem="4G",
        cpus_per_task=2
    run:
        import morfessor
        import sentencepiece as spm

        morf_model = morfessor.MorfessorIO().read_binary_model_file(input.morf_model)
        sp_model = spm.SentencePieceProcessor(model_file=input.sp_model)

        with open(input.ftvocab, "r") as f, open(output[0], "w") as g:
            for line in f:
                word = line.split()[0]
                morf_tokens = morf_model.viterbi_segment(word)[0]
                sp_tokens = []
                for morf_token in morf_tokens:
                    sp_tokens.extend(
                        tok[1:] if tok.startswith("▁") else tok
                        for tok in sp_model.encode_as_pieces(morf_token))
                if word.startswith("▁") and sp_tokens:
                    sp_tokens[0] = "▁" + sp_tokens[0]
                print(word, " ".join(sp_tokens), file=g)


rule subword_embeddings:
    input:
        allowed="fasttext/allowed.{pretok}-{segm_type}-{kwords}k.{lng}.init",
        pseudoinverse="fasttext/{lng}.ft.out_inv.txt",
        word_embeddings="fasttext/{lng}.ft.txt",
        data="pretokenized/train.{lng}"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    params:
        epochs=10,
    output:
        "{pretok}-{segm_type}-{kwords}k-embedding.{lng}/segmentations.9",
        "{pretok}-{segm_type}-{kwords}k-embedding.{lng}/subwords.9",
        "{pretok}-{segm_type}-{kwords}k-embedding.{lng}/unigram_stats.9",
        "{pretok}-{segm_type}-{kwords}k-embedding.{lng}/bigram_stats.9",
    shell:
        """
        DIR=`dirname {output[0]}`
        mkdir -p $DIR
        {subword_segmentation}/build/train_subword_embeddings \
            {input.word_embeddings} \
            {input.data} \
            --fastext-output-pseudoinverse {input.pseudoinverse} \
            --allowed-substrings {input.allowed} \
            --epochs {params.epochs} \
            --output-directory $DIR
        rm -f $DIR/*.{{1,2,3,4,5,6,7,8}}
        """


rule bigram_segment:
    input:
        bigram_stats="{pretok}-{segm_type}-{kwords}k-embedding.{lng}/bigram_stats.9",
        unigram_stats="{pretok}-{segm_type}-{kwords}k-embedding.{lng}/unigram_stats.9",
        data="pretokenized/{split}.{lng}"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    output:
        "{pretok}-{segm_type}-{kwords}k-embedding/{split}.{lng}"
    shell:
        # TODO change this to use the Python code with beam search segmentation
        """
        mkdir -p `dirname {output}`
        {subword_segmentation}/build/bigram_segment \
            {input.bigram_stats} \
            {input.unigram_stats} \
            < {input.data} | sed 's/@@//g' > {output}
        """


rule train_marian:
    input:
        train_src="{pretok}-{segm_type}-{kwords}k-{segmentation}/train.{src}",
        train_tgt="{pretok}-{segm_type}-{kwords}k-{segmentation}/train.{tgt}",
        dev_src="{pretok}-{segm_type}-{kwords}k-{segmentation}/dev.{src}",
        dev_tgt="{pretok}-{segm_type}-{kwords}k-{segmentation}/dev.{tgt}",
    output:
        model=protected("models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/model.npz"),
        vocab=protected("models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/vocab.yml"),
        decoder_config=protected("models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/model.npz.decoder.yml"),
        log=protected("models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/train.log"),
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:2"
    shell:
        """
        set -ex
        DIR=`dirname {output.model}`
        mkdir -p $DIR
        hostname
        #{marian_home}-dll-3gpu1/marian
        {marian_home}-`hostname`/marian \
            --model {output.model} \
            --log {output.log} \
            --vocabs {output.vocab} {output.vocab} \
            --config config.yml \
            --devices 0 1 \
            --train-sets {input.train_src} {input.train_tgt} \
            --valid-sets {input.dev_src} {input.dev_tgt}
        rm $DIR/model.npz.optimizer.npz
        """


rule translate:
    input:
        decoder_config="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/model.npz.decoder.yml",
        test_src="{pretok}-{segm_type}-{kwords}k-{segmentation}/{split}.{src}",
    output:
        test_out="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/{split}.out"
    resources:
        mem="120G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    shell:
        """
        set -ex
        #{marian_home}-dll-3gpu1/marian
        {marian_home}-`hostname`/marian-decoder \
            --config {input.decoder_config} \
            < {input.test_src} | \
            sed 's/ //g;s/▁/ /g;s/^ *//g;s/ *$//g' \
            > {output.test_out}
        """


rule evaluate:
    input:
        test_out="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/{split}.out",
        test_reference="plain_data/{split}.{tgt}"
    output:
        bleu="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/{split}.bleu",
        chrf="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/{split}.chrf"
    run:
        import sacrebleu
        with open(input.test_out, encoding="utf8", errors="ignore") as f:
            hyps = [line.strip() for line in f]
        with open(input.test_reference, encoding="utf8", errors="ignore") as f:
            refs = [line.strip() for line in f]

        bleu = sacrebleu.corpus_bleu(hyps, [refs])
        chrf = sacrebleu.corpus_chrf(hyps, [refs])

        with open(output.bleu, "w") as f:
            f.write(str(bleu.score))
        with open(output.chrf, "w") as f:
            f.write(str(chrf.score))


rule done:
    input:
        test_bleu="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/test.bleu",
        test_chrf="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/test.chrf",
        dev_bleu="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/dev.bleu",
        dev_chrf="models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/dev.chrf",
    output:
        touch("models/{src}-{tgt}-{pretok}-{segm_type}-{kwords}k-{segmentation}/done")
