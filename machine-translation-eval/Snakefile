subword_segmentation="/lnet/troja/projects/neuralpiece/subword-segmentation"
sentencepiece="/lnet/troja/projects/neuralpiece/evaluation/sentencepiece/build/src"
marian_home="/lnet/troja/projects/hplt/marian-dev/build"


#VOCAB_SIZES = [8, 16, 32]
VOCAB_SIZES = [8]
#TYPES = ["bpe", "unigram"]
TYPES = ["bpe"]
EXPERIMENTS = ["embedding", "original"]
PRETOKS = ["words", "morf"]

import glob


rule joint_sentencepiece_model:
    input:
        glob.glob("plain_data/train.*")
    output:
        "subword-{pretok}-{model_type}-{kwords}k/{model_type}.{kwords}k.model",
        "subword-{pretok}-{model_type}-{kwords}k/{model_type}.{kwords}k.vocab",
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    shell:
        """
        DIR=subword-{wildcards.pretok}-{wildcards.model_type}-{wildcards.kwords}k
        mkdir -p $DIR
        {sentencepiece}/spm_train \
            --input={input[0]},{input[1]} \
            --model_prefix=$DIR/{wildcards.model_type}.{wildcards.kwords}k \
            --vocab_size={wildcards.kwords}000 \
            --model_type={wildcards.model_type} \
            --num_threads={threads} \
            --input_sentence_size=10000000 \
            --shuffle_input_sentence=true \
        """


rule apply_sentencepiece:
    input:
        data="plain_data/{split}.{lng}",
        model="subword-words-{model_type}-{kwords}k/{model_type}.{kwords}k.model",
    output:
        "subword-words-{model_type}-{kwords}k/{split}.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
    shell:
        "cat {input.data} | {sentencepiece}/spm_encode --model={input.model} > {output}"


def pretokenize_line(line):
    char_list = ["▁"]
    for char in line.strip():
        if char == " ":
            char_list.append(" ")
            char_list.append("▁")
            continue
        if char_list[-1] != "▁" and char_list[-1].isalpha() and not char.isalpha():
            char_list.append(" ")
        if char_list[-1] != "▁" and not char_list[-1].isalpha() and char.isalpha():
            char_list.append(" ")
        char_list.append(char)
    return "".join(char_list)


rule pretokenize:
    input:
        "plain_data/{split}.{lng}"
    output:
        "pretokenized/{split}.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
    run:
       with open(input[0], "r") as f:
            with open(output[0], "w") as g:
                for line in f:
                    print(pretokenize_line(line), file=g)


rule morfessor:
    input:
        "pretokenized/train.{lng}"
    output:
        "morfessor/{lng}.model"
    resources:
        mem="4G",
        cpus_per_task=2
    shell:
        """
        mkdir -p morfessor
        morfessor-train \
            --encoding=UTF-8 \
            --logfile=morfessor/training.log \
            --save morfessor/model.bin \
            -d ones \
            {input}
        """


rule sentencepiece_over_morfessor:
    input:
        "morfessor/{src}.model",
        "morfessor/{tgt}.model",
        "pretokenized/train.{src}",
        "pretokenized/train.{tgt}"
    output:
        morf_pretokenized_src="morfessor/train.{src}",
        morf_pretokenized_tgt="morfessor/train.{tgt}",
        model="subword-morf-{model_type}-{kwords}k/{model_type}.{kwords}k.model",
        vocab="subword-morf-{model_type}-{kwords}k/{model_type}.{kwords}k.vocab",
    resources:
        mem="40G",
        cpus_per_task=16
    run:
        import morfessor
        import sentencepiece as spm

        io = morfessor.MorfessorIO()
        model_src = io.read_binary_model_file(input[0])
        model_tgt = io.read_binary_model_file(input[1])

        with open(input[2], "r") as f:
            with open(output.morf_pretokenized_src, "w") as g:
                for line in f:
                    line_tokens = []
                    for word in pretokenize_line(line.strip()).split():
                        line_tokens.extend(model_src.viterbi_segment(word)[0])
                    print(" ".join(line_tokens), file=g)

        with open(input[3], "r") as f:
            with open(output.morf_pretokenized_tgt, "w") as g:
                for line in f:
                    line_tokens = []
                    for word in pretokenize_line(line.strip()).split():
                        line_tokens.extend(model_tgt.viterbi_segment(word)[0])
                    print(" ".join(line_tokens), file=g)

        spm.SentencePieceTrainer.train(
            input=[output.morf_pretokenized_src, output.morf_pretokenized_tgt],
            model_prefix=output.model,
            vocab_size=wildcards.kwords * 1000,
            model_type=wildcards.model_type,
            input_sentence_size=10000000,
            shuffle_input_sentence=True,
            num_threads=16
        )


rule apply_morfessor_and_sentencepiece:
    input:
        data="plain_data/{split}.{lng}",
        morf_model="morfessor/{lng}.model",
        sp_model="subword-morf-{model_type}-{kwords}k/{model_type}.{kwords}k.model",
    output:
        "subword-morf-{model_type}-{kwords}k/{split}.{lng}"
    resources:
        mem="4G",
        cpus_per_task=2
        run:
            import morfessor
            import sentencepiece as spm

            io = morfessor.MorfessorIO()
            morf_model = io.read_binary_model_file(input.morf_model)
            sp_model = spm.SentencePieceProcessor(model_file=input.sp_model)

            with open(input.data, "r") as f, open(output[0], "w") as g:
                for line in f:
                    line_tokens = []
                    pretok = pretokenize_line(line)
                    for token in pretok.split():
                        if token.startswith("▁"):
                            prefix = "▁"
                            token = token[1:]
                        else:
                            prefix = ""
                        morf_tokens = morf_model.viterbi_segment(token)
                        sp_tokens = []
                        for morf_token in morf_tokens:
                            sp_tokens.extend(
                                tok[1:] if tok.startswith("▁") else tok
                                for tok in sp_model.encode_as_pieces(morf_token))
                        sp_tokens[0] = prefix + sp_tokens[0]
                        line_tokens.extend(sp_tokens)
                    print(" ".join(line_tokens), file=g)


rule fasttext:
    input:
        "pretokenized/train.{lng}"
    params:
        prefix="fasttext/{lng}.ft"
    output:
        "fasttext/{lng}.ft",
        "fasttext/{lng}.ft.vocab",
        "fasttext/{lng}.ft.out_inv.txt",
        "fasttext/{lng}.ft.txt"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    shell:
        """
        mkdir -p fasttext
        {subword_segmentation}/scripts/train_fasttext.py --epochs 30 --num-threads {threads} {input} {params.prefix}
        """

rule allowed_init_list_from_sentencepiece:
    input:
        model="subword-words-{model_type}-{kwords}k/{model_type}.{kwords}k.model",
        ftvocab="fasttext/{lng}.ft.vocab"
    output:
        "fasttext/allowed.words-{model_type}-{kwords}k.{lng}.init"
    resources:
        mem="4G",
        cpus_per_task=2
    shell:
        """
        paste {input.ftvocab} <(
                sed 's/^▁//' {input.ftvocab} | \
                {sentencepiece}/spm_encode --model={input.model} ) \
        > {output}
        """

# TODO allowed init with morfessor pretok

rule subword_embeddings:
    input:
        allowed="fasttext/allowed.{pretok}-{segm_type}-{kwords}k.{lng}.init",
        pseudoinverse="fasttext/{lng}.ft.out_inv.txt",
        word_embeddings="fasttext/{lng}.ft.txt",
        data="pretokenized/train.{lng}"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    params:
        epochs=10,
        workdir=lambda wildcards, output: "subword_embs-{}-{}-{}k.{}".format(
            wildcards.pretok, wildcards.segm_type,
            wildcards.kwords, wildcards.lng)
    output:
        "subword_embs-{pretok}-{segm_type}-{kwords}k.{lng}/segmentations.9",
        "subword_embs-{pretok}-{segm_type}-{kwords}k.{lng}/subwords.9",
        "subword_embs-{pretok}-{segm_type}-{kwords}k.{lng}/unigram_stats.9",
        "subword_embs-{pretok}-{segm_type}-{kwords}k.{lng}/bigram_stats.9",
    shell:
        # note this assumes params.workdir is a direct subdirectory
        """
        mkdir -p {params.workdir}
        {subword_segmentation}/build/train_subword_embeddings \
            {input.word_embeddings} \
            {input.data} \
            --fastext-output-pseudoinverse {input.pseudoinverse} \
            --allowed-substrings {input.allowed} \
            --epochs {params.epochs} \
            --output-directory {params.workdir}
        rm -f {params.workdir}/*.{1,2,3,4,5,6,7,8}
        """


rule bigram_segment:
    input:
        bigram_stats="subword_embs-{pretok}-{segm_type}-{kwords}k.{lng}/bigram_stats.9",
        unigram_stats="subword_embs-{pretok}-{segm_type}-{kwords}k.{lng}/unigram_stats.9",
        data="pretokenized/{split}.{lng}"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    output:
        "ours-{segm_type}-{kwords}k/{split}.{lng}"
    shell:
        # TODO change this to use the Python code with beam search segmentation
        """
        mkdir -p segmented.{wildcards.segm_type}-{wildcards.kwords}k
        {subword_segmentation}/build/bigram_segment \
            {input.bigram_stats} \
            {input.unigram_stats} \
            < {input.data} | sed 's/@@//g' > {output}
        """


rule train_marian:
    input:
        train_src="{segmentation}-{pretok}-{segm_type}-{kwords}k/train.{src}",
        train_tgt="{segmentation}-{pretok}-{segm_type}-{kwords}k/train.{tgt}",
        dev_src="{segmentation}-{pretok}-{segm_type}-{kwords}k/dev.{src}",
        dev_tgt="{segmentation}-{pretok}-{segm_type}-{kwords}k/dev.{tgt}",
    output:
        model="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/model.npz",
        vocab="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/vocab.yml",
        decoder_config="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/model.npz.decoder.yml",
        log="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/train.log",
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:2"
    shell:
        """
        mkdir -p models/{wildcards.src}-{wildcards.tgt}-{wildcards.segmentation}-{wildcards.pretok}-{wildcards.segm_type}-{wildcards.kwords}k
        hostname
        {marian_home}-`hostname`/marian \
            --model {output.model} \
            --log {output.log} \
            --vocabs {output.vocab} {output.vocab} \
            --config config.yml \
            --devices 0 1 \
            --train-sets {input.train_src} {input.train_tgt} \
            --valid-sets {input.dev_src} {input.dev_tgt}
        """


rule translate:
    input:
        decoder_config="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/model.npz.decoder.yml",
        test_src="{segmentation}-{pretok}-{segm_type}-{kwords}k/{split}.{src}",
    output:
        test_out="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/{split}.out"
    resources:
        mem="120G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    shell:
        """
        {marian_home}-`hostname`/marian-decoder \
            --config {input.decoder_config} \
            < {input.test_src} | \
            sed 's/ //g;s/▁/ /g;s/^ *//g;s/ *$//g' \
            > {output.test_out}
        """


rule evaluate:
    input:
        test_out="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/{split}.out",
        test_reference="plain_data/{split}.{tgt}"
    output:
        bleu="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/{split}.bleu",
        chrf="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/{split}.chrf"
    run:
        import sacrebleu
        with open(input.test_out, encoding="utf8", errors="ignore") as f:
            hyps = [line.strip() for line in f]
        with open(input.test_reference, encoding="utf8", errors="ignore") as f:
            refs = [line.strip() for line in f]

        bleu = sacrebleu.corpus_bleu(hyps, [refs])
        chrf = sacrebleu.corpus_chrf(hyps, [refs])

        with open(output.bleu, "w") as f:
            f.write(str(bleu.score))
        with open(output.chrf, "w") as f:
            f.write(str(chrf.score))


rule done:
    input:
        test_bleu="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/test.bleu",
        test_chrf="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/test.chrf",
        dev_bleu="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/dev.bleu",
        dev_chrf="models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/dev.chrf",
    output:
        touch("models/{src}-{tgt}-{segmentation}-{pretok}-{segm_type}-{kwords}k/done")
