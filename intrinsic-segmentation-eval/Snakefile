legros_home = "../legros"
spm_train = "../3rd_party/sentencepiece/build/src/spm_train"
spm_encode = "../3rd_party/sentencepiece/build/src/spm_encode"
decompress = "pigz -cd"
compress = "pigz -c"


lng2to3 = {
    "cs": "ces",
    "en": "eng",
    "es": "spa",
    "fr": "fra",
    "hu": "hun",
    "it": "ita",
    "mn": "mon",
    "ru": "rus"}

rule all:
    #input: expand("{lng}/fasttext", lng=["cs", "en", "es", "fr", "hu", "it", "mn", "ru"])
    input: expand("{lng}/fasttext", lng=["cs"])

rule download_data_mn:
    output: "mn/plaintext/mn.raw.txt.gz"
    threads: 8
    resources:
        mem="32G",
        cpus_per_task=8
    shell:
        """
        DIR=`dirname {output}`
        mkdir -p $DIR

        wget https://data.statmt.org/cc-100/mn.txt.xz -O $DIR/mn.txt.xz
        xzcat $DIR/mn.txt.xz | {compress} > {output}
        rm $DIR/mn.txt.xz
        """

rule download_data:
    output: "{lng}/plaintext/{lng}.raw.txt.gz"
    wildcard_constraints:
        lng="cs|en|es|fr|hu|it|ru"
    threads: 8
    resources:
        mem="32G",
        cpus_per_task=8
    params:
        datasize=5000
    shell:
        """
        DIR=`dirname {output}`
        echo making $DIR
        mkdir -p $DIR
        SIZE=0

        # Download years in decreasing order, until we reach 50M sentences

        for YEAR in {{2021..2007}}; do
            wget https://data.statmt.org/news-crawl/{wildcards.lng}/news.$YEAR.{wildcards.lng}.shuffled.deduped.gz -O $DIR/tmp.newscrawl.$YEAR.gz
            YEAR_SIZE=$({decompress} $DIR/tmp.newscrawl.$YEAR.gz | wc -l)
            SIZE=$(( SIZE + YEAR_SIZE  ))
            if [ $SIZE -gt {params.datasize} ]; then
                break
            fi
        done

        {decompress} $DIR/tmp.newscrawl.*.gz | shuf -n {params.datasize} | {compress} > {output}
        rm $DIR/tmp.newscrawl.*.gz
        """

rule tokenize:
    input: "{lng}/plaintext/{lng}.raw.txt.gz"
    output: "{lng}/plaintext/{lng}.lc.txt"
    threads: 32
    resources:
        mem="64G",
        cpus_per_task=32
    shell:
        "{decompress} {input} | sacremoses -j {threads} -l {wildcards.lng} tokenize -x | sed 's/[[:upper:]]*/\L&/g' > {output}"


rule get_vocabulary:
    input: "{lng}/plaintext/{lng}.lc.txt"
    output:
        vocab="{lng}/plaintext/{lng}.lc.txt.vocab",
        vocab200k="{lng}/plaintext/{lng}.lc.txt.vocab.200k"
    threads: 32
    resources:
        mem="32G",
        cpus_per_task=32
    shell:
        """
        python3 get_vocabulary.py {input} --min-count 5 --num-threads {threads} > {output.vocab}
        head -n 200000 {output.vocab} > {output.vocab200k}
        """

rule train_fasttext:
    input: "{lng}/plaintext/{lng}.lc.txt"
    output:
        model="{lng}/fasttext",
        #aux="{lng}/fasttext.syn1neg.npy",
        vocab="{lng}/fasttext.vocab",
        pinv="{lng}/fasttext.out_inv.txt",
        text="{lng}/fasttext.txt",
        #wvvocab="{lng}/fasttext.wv.vectors_vocab.npy",
        wvngrams="{lng}/fasttext.wv.vectors_ngrams.npy"
    threads: 16
    resources:
        mem="32G",
        cpus_per_task=16
    params:
        dimension=200,
        vocab_size=200000,
        epochs=10
    shell:
        """
        python3 {legros_home}/scripts/train_fasttext.py {input} {output.model} \
            --num-threads {threads} \
            --dimension {params.dimension} \
            --vocab-size {params.vocab_size} \
            --epochs {params.epochs}
        """

rule download_sigmorphon:
    output: "{lng}/sigmorphon_set.tsv"
    params:
        baseurl="https://raw.githubusercontent.com/sigmorphon/2022SegmentationST/main/data/",
        lng3=lambda wildcards, output: lng2to3[wildcards.lng]
    shell:
        """
        curl -L '{params.baseurl}/{params.lng3}.word.test.gold.tsv' | \
            python filter_sigmorphon_test_set.py > {output}
        """

rule train_morfessor:
    input: "{lng}/plaintext/{lng}.lc.txt.vocab"
    output:
        model="{lng}/morfessor/model.bin",
        log="{lng}/morfessor/training.log"
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        mkdir -p {wildcards.lng}/morfessor
        morfessor-train \
            --encoding=UTF-8 \
            --traindata-list \
            --logfile {output.log} \
            --save {output.model} \
            -d ones \
            {input}
        """

rule run_morfessor:
    input:
        model="{lng}/morfessor/model.bin",
        data="{lng}/plaintext/{lng}.lc.txt"
    output: "{lng}/plaintext/{lng}.lc.morfessor.txt"
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        python morfessor_pretok.py {input.model} {input.data} > {output}
        """

def resolve_pretok_input(wildcards):
    if wildcards.pretok == "word":
        return "{lng}/plaintext/{lng}.lc.txt"
    elif wildcards.pretok == "morf":
        return "{lng}/plaintext/{lng}.lc.morfessor.txt"
    raise ValueError("unsupported pretok")

rule train_bpe:
    input: resolve_pretok_input
    output: "{lng}/bpe/{lng}.{pretok}-bpe{size_k}"
    wildcard_constraints:
        pretok="word|morf"
    resources:
        mem="50G",
        cpus_per_task=1
    shell:
        """
        mkdir -p {wildcards.lng}/bpe
        subword-nmt learn-bpe -i {input} -o {output} -s {wildcards.size_k}000
        """

rule train_spm:
    input: resolve_pretok_input
    output:
        model="{lng}/spm/{lng}.{pretok}-spm{size_k}.model",
        vocab="{lng}/spm/{lng}.{pretok}-spm{size_k}.vocab"
    wildcard_constraints:
        pretok="word|morf"
    params:
        prefix=lambda wildcards, output: output.model[:-6],
        sentence_size=50000000
    threads: 16
    resources:
        mem="60G",
        cpus_per_task=16
    shell:
        """
        mkdir -p {wildcards.lng}/spm
        {spm_train} \
            --input {input} \
            --model_prefix {params.prefix} \
            --num_threads {threads} \
            --input_sentence_size {params.sentence_size} \
            --vocab_size {wildcards.size_k}000 \
            --train_extremely_large_corpus=true
        """

rule allowed_init_from_morfessor:
    input:
        model="{lng}/morfessor/model.bin",
        vocab="{lng}/fasttext.vocab"
    output: "{lng}/experiments/from_morfessor/init.allowed"
    params:
        expdir=lambda wildcards, output: output[0][:-13]
    shell:
        """
        mkdir -p {params.expdir}
        morfessor-segment - -l {input.model} < {input.vocab} \
            | paste {input.vocab} - > {output}
        """


def resolve_allowed_init_input(wildcards):
    # when initializing allowed_init from morfessor, we go from its allowed init
    if wildcards.pretok == "morf":
        return "{lng}/experiments/from_morfessor/init.allowed".format(lng=wildcards.lng)
    elif wildcards.pretok == "word":
        return "{lng}/fasttext.vocab".format(lng=wildcards.lng)
    raise ValueError("unsupported pretok")


rule allowed_init_from_bpe:
    input:
        pretok_vocab=resolve_allowed_init_input,
        vocab="{lng}/fasttext.vocab",
        model="{lng}/bpe/{lng}.{pretok}-bpe{size_k}",
    output: "{lng}/experiments/from_{pretok}-bpe-{size_k}k/init.allowed"
    params:
        expdir=lambda wildcards, output: output[0][:-13]
    wildcard_constraints:
        pretok="word|morf"
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        mkdir -p {params.expdir}
        if [ {wildcards.pretok} = "word" ]; then
            cat {input.pretok_vocab}
        else
            cut -f2 {input.pretok_vocab}
        fi | \
            subword-nmt apply-bpe -c {input.model} | \
            sed 's/@@ / /g' | \
            paste {input.vocab} - > {output}
        """

rule allowed_init_from_spm:
    input:
        pretok_vocab=resolve_allowed_init_input,
        vocab="{lng}/fasttext.vocab",
        model="{lng}/spm/{lng}.{pretok}-spm{size_k}.model",
    output: "{lng}/experiments/from_{pretok}-spm-{size_k}k/init.allowed"
    params:
        expdir=lambda wildcards, output: output[0][:-13]
    wildcard_constraints:
        pretok="word|morf"
    threads: 2
    resources:
        mem="10G",
        cpus_per_task=2
    shell:
        """
        mkdir -p {params.expdir}
        if [ {wildcards.pretok} = "word" ]; then
            cat {input.pretok_vocab}
        else
            cut -f2 {input.pretok_vocab}
        fi | \
            {spm_encode} --model {input.model} | \
            sed 's/â–//g' | \
            paste -d' ' {input.vocab} - > {output}
        """

rule train_legros:
    input:
        data="{lng}/plaintext/{lng}.lc.txt",
        ft_emb_text="{lng}/fasttext.txt",
        ft_pinv="{lng}/fasttext.out_inv.txt",
        allowed="{lng}/experiments/from_{pretok}-{segm}-{size_k}k/init.allowed"
    output:
        "{lng}/experiments/from_{pretok}-{segm}-{size_k}k/segmentations.19",
        "{lng}/experiments/from_{pretok}-{segm}-{size_k}k/subwords.19",
        "{lng}/experiments/from_{pretok}-{segm}-{size_k}k/unigram_stats.19",
        "{lng}/experiments/from_{pretok}-{segm}-{size_k}k/bigram_stats.19"
    wildcard_constraints:
        pretok="word|morf",
        segm="spm|bpe"
    threads: 60
    resources:
        mem="120G",
        cpus_per_task=60
    params:
        epochs=20,
        output_dir=lambda wildcards, output: output[0][:-17]
    shell:
        """
        {legros_home}/build/legros-train \
            {input.ft_emb_text} \
            {input.data} \
            --fastext-output-pseudoinverse {input.ft_pinv} \
            --allowed-substrings {input.allowed} \
            --epochs {params.epochs} \
            --output-directory {params.output_dir}

        rm {params.output_dir}/{{segmentations,subwords,unigram_stats,bigram_stats}}.{{1..18}}
        """
