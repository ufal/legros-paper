legros_home = "../legros"
spm_train = "../3rd_party/sentencepiece/build/src/spm_train"
spm_encode = "../3rd_party/sentencepiece/build/src/spm_encode"
decompress = "pigz -cd"
compress = "pigz -c"


lng2to3 = {
    "cs": "ces",
    "en": "eng",
    "es": "spa",
    "fr": "fra",
    "hu": "hun",
    "it": "ita",
    "mn": "mon",
    "ru": "rus"}


all_langs = ["cs", "en", "es", "fr", "hu", "it", "mn", "ru"]
segm_types = ["spm", "bpe"]
pretok_types = ["word", "morf"]
vocab_sizes_k = [1, 2, 4, 8, 16, 24, 32, 48]
evaluations = ["embedding-based", "bigrams"]

rule all:
    input:
        expand("{lng}/experiments/from_{pretok}-{segm}-{size_k}k/sigmorphon.{method}.fscore",
            lng=all_langs, pretok=pretok_types, segm=segm_types, size_k=vocab_sizes_k, method=evaluations),
        expand("{lng}/experiments/from_morfessor/sigmorphon.{method}.fscore",
            lng=all_langs, method=evaluations)
            

    #input: expand("{lng}/morfessor/model.bin", lng=all_langs)
    #input: expand("{lng}/fasttext", lng=["cs"])
    # input: expand("{lng}/experiments/from_{pretok}-{segm}-{size_k}k/subwords.19", \
    #               lng=all_langs, \
    #               pretok=pretok_types, \
    #               segm=segm_types, \
    #               size_k=vocab_sizes_k)
    #
    #input: expand("{lng}/experiments/from_{pretok}-{segm}-{size_k}k/bigram_stats", \
    #              lng=all_langs, pretok=pretok_types, segm=segm_types, size_k=vocab_sizes_k)
    #input: "cs/experiments/from_word-bpe-1k/sigmorphon.bigrams.fscore"

rule download_data_mn:
    output: "mn/plaintext/mn.raw.txt.gz"
    threads: 8
    resources:
        mem="32G",
        cpus_per_task=8
    shell:
        """
        DIR=`dirname {output}`
        mkdir -p $DIR

        wget https://data.statmt.org/cc-100/mn.txt.xz -O $DIR/mn.txt.xz
        xzcat $DIR/mn.txt.xz | {compress} > {output}
        rm $DIR/mn.txt.xz
        """

rule download_data:
    output: "{lng}/plaintext/{lng}.raw.txt.gz"
    wildcard_constraints:
        lng="cs|en|es|fr|hu|it|ru"
    threads: 8
    resources:
        mem="32G",
        cpus_per_task=8
    params:
        datasize=50000000
    shell:
        """
        DIR=`dirname {output}`
        echo making $DIR
        mkdir -p $DIR
        SIZE=0

        # Download years in decreasing order, until we reach 50M sentences

        for YEAR in {{2021..2007}}; do
            wget https://data.statmt.org/news-crawl/{wildcards.lng}/news.$YEAR.{wildcards.lng}.shuffled.deduped.gz -O $DIR/tmp.newscrawl.$YEAR.gz
            YEAR_SIZE=$({decompress} $DIR/tmp.newscrawl.$YEAR.gz | wc -l)
            SIZE=$(( SIZE + YEAR_SIZE  ))
            if [ $SIZE -gt {params.datasize} ]; then
                break
            fi
        done

        {decompress} $DIR/tmp.newscrawl.*.gz | shuf -n {params.datasize} | {compress} > {output}
        rm $DIR/tmp.newscrawl.*.gz
        """

rule tokenize:
    input: "{lng}/plaintext/{lng}.raw.txt.gz"
    output: "{lng}/plaintext/{lng}.lc.txt"
    threads: 32
    resources:
        mem="64G",
        cpus_per_task=32
    shell:
        "{decompress} {input} | sacremoses -j {threads} -l {wildcards.lng} tokenize -x | sed 's/[[:upper:]]*/\L&/g' > {output}"


rule get_vocabulary:
    input: "{lng}/plaintext/{lng}.lc.txt"
    output:
        vocab="{lng}/plaintext/{lng}.lc.txt.vocab",
        vocab200k="{lng}/plaintext/{lng}.lc.txt.vocab.200k"
    threads: 32
    resources:
        mem="32G",
        cpus_per_task=32
    shell:
        """
        python3 get_vocabulary.py {input} --min-count 5 --num-threads {threads} > {output.vocab}
        head -n 200000 {output.vocab} > {output.vocab200k}
        """

rule train_fasttext:
    input: "{lng}/plaintext/{lng}.lc.txt"
    output:
        model="{lng}/fasttext",
        #aux="{lng}/fasttext.syn1neg.npy",
        vocab="{lng}/fasttext.vocab",
        pinv="{lng}/fasttext.out_inv.txt",
        text="{lng}/fasttext.txt",
        #wvvocab="{lng}/fasttext.wv.vectors_vocab.npy",
        wvngrams="{lng}/fasttext.wv.vectors_ngrams.npy"
    threads: 16
    resources:
        mem="32G",
        cpus_per_task=16
    params:
        dimension=200,
        vocab_size=200000,
        epochs=10
    shell:
        """
        python3 {legros_home}/scripts/train_fasttext.py {input} {output.model} \
            --num-threads {threads} \
            --dimension {params.dimension} \
            --vocab-size {params.vocab_size} \
            --epochs {params.epochs}
        """

rule download_sigmorphon:
    output: "{lng}/sigmorphon_set.tsv"
    params:
        baseurl="https://raw.githubusercontent.com/sigmorphon/2022SegmentationST/main/data/",
        lng3=lambda wildcards, output: lng2to3[wildcards.lng]
    shell:
        """
        curl -L '{params.baseurl}/{params.lng3}.word.test.gold.tsv' | \
            python filter_sigmorphon_test_set.py > {output}
        """

rule train_morfessor:
    input: "{lng}/plaintext/{lng}.lc.txt.vocab"
    output:
        model="{lng}/morfessor/model.bin",
        log="{lng}/morfessor/training.log"
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        mkdir -p {wildcards.lng}/morfessor
        morfessor-train \
            --encoding=UTF-8 \
            --traindata-list \
            --logfile {output.log} \
            --save {output.model} \
            -d ones \
            {input}
        """

rule run_morfessor:
    input:
        model="{lng}/morfessor/model.bin",
        data="{lng}/plaintext/{lng}.lc.txt"
    output: "{lng}/plaintext/{lng}.lc.morfessor.txt"
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        python morfessor_pretok.py {input.model} {input.data} > {output}
        """

def resolve_pretok_input(wildcards):
    if wildcards.pretok == "word":
        return "{lng}/plaintext/{lng}.lc.txt"
    elif wildcards.pretok == "morf":
        return "{lng}/plaintext/{lng}.lc.morfessor.txt"
    raise ValueError("unsupported pretok")

rule train_bpe:
    input: resolve_pretok_input
    output: "{lng}/bpe/{lng}.{pretok}-bpe{size_k}"
    wildcard_constraints:
        pretok="word|morf"
    resources:
        mem="50G",
        cpus_per_task=1
    shell:
        """
        mkdir -p {wildcards.lng}/bpe
        subword-nmt learn-bpe -i {input} -o {output} -s {wildcards.size_k}000
        """

rule train_spm:
    input: resolve_pretok_input
    output:
        model="{lng}/spm/{lng}.{pretok}-spm{size_k}.model",
        vocab="{lng}/spm/{lng}.{pretok}-spm{size_k}.vocab"
    wildcard_constraints:
        pretok="word|morf"
    params:
        prefix=lambda wildcards, output: output.model[:-6],
        #sentence_size=lambda wildcards, output: 5000000 if wildcards.pretok == "word" else 100000000 # this used to be 50M, but we reduce it to 5M in case of word
        sentence_size=5000000  # turns out morf pretok also OOMs.
    threads: 32
    resources:
        mem="200G",
        cpus_per_task=32
    shell:
        """
        mkdir -p {wildcards.lng}/spm
        {spm_train} \
            --input {input} \
            --model_prefix {params.prefix} \
            --num_threads {threads} \
            --input_sentence_size {params.sentence_size} \
            --vocab_size {wildcards.size_k}000 \
            --train_extremely_large_corpus=true
        """

rule allowed_init_from_morfessor:
    input:
        model="{lng}/morfessor/model.bin",
        vocab="{lng}/fasttext.vocab"
    output: "{lng}/experiments/from_morfessor/init.allowed"
    params:
        expdir=lambda wildcards, output: output[0][:-13]
    shell:
        """
        mkdir -p {params.expdir}
        morfessor-segment - -l {input.model} < {input.vocab} \
            | paste {input.vocab} - > {output}
        """


def resolve_allowed_init_input(wildcards):
    # when initializing allowed_init from morfessor, we go from its allowed init
    if wildcards.pretok == "morf":
        return "{lng}/experiments/from_morfessor/init.allowed".format(lng=wildcards.lng)
    elif wildcards.pretok == "word":
        return "{lng}/fasttext.vocab".format(lng=wildcards.lng)
    raise ValueError("unsupported pretok")


rule allowed_init_from_bpe:
    input:
        pretok_vocab=resolve_allowed_init_input,
        vocab="{lng}/fasttext.vocab",
        model="{lng}/bpe/{lng}.{pretok}-bpe{size_k}",
    output: "{lng}/experiments/from_{pretok}-bpe-{size_k}k/init.allowed"
    params:
        expdir=lambda wildcards, output: output[0][:-13]
    wildcard_constraints:
        pretok="word|morf"
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        mkdir -p {params.expdir}
        if [ {wildcards.pretok} = "word" ]; then
            cat {input.pretok_vocab}
        else
            cut -f2 {input.pretok_vocab}
        fi | \
            subword-nmt apply-bpe -c {input.model} | \
            sed 's/@@ / /g' | \
            paste {input.vocab} - > {output}
        """

rule allowed_init_from_spm:
    input:
        pretok_vocab=resolve_allowed_init_input,
        vocab="{lng}/fasttext.vocab",
        model="{lng}/spm/{lng}.{pretok}-spm{size_k}.model",
    output: "{lng}/experiments/from_{pretok}-spm-{size_k}k/init.allowed"
    params:
        expdir=lambda wildcards, output: output[0][:-13]
    wildcard_constraints:
        pretok="word|morf"
    threads: 2
    resources:
        mem="10G",
        cpus_per_task=2
    shell:
        """
        mkdir -p {params.expdir}
        if [ {wildcards.pretok} = "word" ]; then
            cat {input.pretok_vocab}
        else
            cut -f2 {input.pretok_vocab}
        fi | \
            {spm_encode} --model {input.model} | \
            sed 's/â–//g' | \
            paste -d' ' {input.vocab} - > {output}
        """

rule train_legros:
    input:
        data="{lng}/plaintext/{lng}.lc.txt",
        ft_emb_text="{lng}/fasttext.txt",
        ft_pinv="{lng}/fasttext.out_inv.txt",
        allowed="{lng}/experiments/from_{experiment}/init.allowed"
    output:
        "{lng}/experiments/from_{experiment}/segmentations.19",
        "{lng}/experiments/from_{experiment}/subwords.19",
        "{lng}/experiments/from_{experiment}/unigram_stats.19",
        "{lng}/experiments/from_{experiment}/bigram_stats.19",
        "{lng}/experiments/from_{experiment}/subword_embeddings.19",
    threads: 60
    resources:
        mem="250G",
        cpus_per_task=60
    params:
        epochs=20,
        output_dir=lambda wildcards, output: output[0][:-17]
    shell:
        """
        {legros_home}/build/legros-train \
            {input.ft_emb_text} \
            {input.data} \
            --fastext-output-pseudoinverse {input.ft_pinv} \
            --allowed-substrings {input.allowed} \
            --epochs {params.epochs} \
            --output-directory {params.output_dir}

        rm {params.output_dir}/{{segmentations,subwords,unigram_stats,bigram_stats,subword_embeddings}}.{{1..18}}
        """

rule distill_bigram:
    input:
        ft_model="{lng}/fasttext",
        subwords="{lng}/experiments/from_{experiment}/subwords.19",
        embeddings="{lng}/experiments/from_{experiment}/subword_embeddings.19",
        vocab200k="{lng}/plaintext/{lng}.lc.txt.vocab.200k"
    output: "{lng}/experiments/from_{experiment}/bigram_stats"
    threads: 1
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        python {legros_home}/scripts/distill_count_based_bigram_model.py \
            {input.ft_model} \
            {input.subwords} \
            {input.embeddings} \
            {input.vocab200k} \
            {output}
        """

rule evaluate_embedding_based:
    input:
        testset="{lng}/sigmorphon_set.tsv",
        subwords="{lng}/experiments/from_{experiment}/subwords.19",
        subword_embeddings="{lng}/experiments/from_{experiment}/subword_embeddings.19",
        fasttext="{lng}/fasttext"
    output:
        score_file="{lng}/experiments/from_{experiment}/sigmorphon.embedding-based.score",
        recall_file="{lng}/experiments/from_{experiment}/sigmorphon.embedding-based.recall",
        fscore_file="{lng}/experiments/from_{experiment}/sigmorphon.embedding-based.fscore",
        output_file="{lng}/experiments/from_{experiment}/sigmorphon.embedding-based.output.tsv"
    threads: 1
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        wc -l < {input.subwords} | tr '\n' ',' > {output.score_file}
        cp {output.score_file} {output.recall_file}
        
        cut -f1 {input.testset} | \
            PYTHONPATH={legros_home} python -m legros.segment_vocab_with_subword_embeddings \
                {input.fasttext} \
                {input.subwords} \
                {input.subword_embeddings} | \
            paste <(cut -f1 {input.testset}) - > {output.output_file}

        python eval_boundary_precision.py --gold {input.testset} --guess {output.output_file} >> {output.score_file}
        python eval_boundary_precision.py --gold {input.testset} --guess {output.output_file} --use-recall >> {output.recall_file}
        python f_score_from_prec_and_recall.py {output.score_file} {output.recall_file} > {output.fscore_file}            
        """

rule evaluate_bigram_based:
    input:
        testset="{lng}/sigmorphon_set.tsv",
        subwords="{lng}/experiments/from_{experiment}/subwords.19",
        bigram_stats="{lng}/experiments/from_{experiment}/bigram_stats",
    output:
        score_file="{lng}/experiments/from_{experiment}/sigmorphon.bigrams.score",
        recall_file="{lng}/experiments/from_{experiment}/sigmorphon.bigrams.recall",
        fscore_file="{lng}/experiments/from_{experiment}/sigmorphon.bigrams.fscore",
        output_file="{lng}/experiments/from_{experiment}/sigmorphon.bigrams.output.tsv"
    threads: 1
    resources:
        mem="10G",
        cpus_per_task=1
    shell:
        """
        wc -l < {input.subwords} | tr '\n' ',' > {output.score_file}
        cp {output.score_file} {output.recall_file}
        
        cut -f1 {input.testset} | \
            PYTHONPATH={legros_home} python -m legros.segment_vocab_with_bigram_stats \
                --greedy {input.bigram_stats} | \
                paste <(cut -f1 {input.testset}) - > {output.output_file}

        python eval_boundary_precision.py --gold {input.testset} --guess {output.output_file} >> {output.score_file}
        python eval_boundary_precision.py --gold {input.testset} --guess {output.output_file} --use-recall >> {output.recall_file}
        python f_score_from_prec_and_recall.py {output.score_file} {output.recall_file} > {output.fscore_file}            
        """
